# Use variables here even if not strictly necessary
variable "asset_dir" {
  type = "string"
}

variable "os_channel" {
  type = "string"
}

cluster "aws" {
  asset_dir        = pathexpand(var.asset_dir)
  cluster_name     = "$CLUSTER_ID"
  controller_count = 2
  dns_zone         = "$AWS_DNS_ZONE"
  dns_zone_id      = "$AWS_DNS_ZONE_ID"
  os_channel       = var.os_channel
  ssh_pubkeys      = ["$PUB_KEY"]
  enable_csi       = true

  controller_clc_snippets = [
    <<EOF
storage:
  files:
    - path: /opt/clc_snippet_hello
      filesystem: root
      contents:
        inline: Hello, world!
      mode: 0644
      user:
        id: 500
      group:
        id: 500
EOF
  ]

  ignore_x509_cn_check = true

  worker_pool "$CLUSTER_ID-w1" {
    count         = 3
    ssh_pubkeys   = ["$PUB_KEY"]
    disk_size     = 30
    instance_type = "i3.large"
    spot_price    = "0.08"
    labels        = {
      "testing.io"          = "yes",
      "roleofnode"          = "testing",
      "conntrack-modified"  = "true",
    }
    tags = {
      "deployment" = "ci"
    }
    clc_snippets = [
      <<EOF
storage:
  files:
    - path: /opt/clc_snippet_hello
      filesystem: root
      contents:
        inline: Hello, world!
      mode: 0644
      user:
        id: 500
      group:
        id: 500
EOF
      ,
      <<EOF
storage:
  files:
    - path: /etc/modules-load.d/nf.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          nf_conntrack
    - path: /etc/sysctl.d/nf.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          net.netfilter.nf_conntrack_max=50000
EOF
      ,
    ]
  }

  worker_pool "$CLUSTER_ID-w2" {
    count         = 1
    ssh_pubkeys   = ["$PUB_KEY"]
    disk_size     = 30
    instance_type = "t2.small"
    spot_price    = "0.01"
    labels        = {
      "testing.io"          = "yes",
      "roleofnode"          = "testing",
      "conntrack-modified"  = "true",
    }
    taints        = {
      "nodeType" = "storage:NoSchedule"
    }

    # TODO: remove this once https://github.com/kinvolk/lokomotive/issues/839 is fixed.
    lb_http_port  = 8080
    lb_https_port = 8443
    tags = {
      "deployment" = "ci"
    }
    clc_snippets = [
      <<EOF
storage:
  files:
    - path: /opt/clc_snippet_hello
      filesystem: root
      contents:
        inline: Hello, world!
      mode: 0644
      user:
        id: 500
      group:
        id: 500
EOF
      ,
      <<EOF
storage:
  files:
    - path: /etc/modules-load.d/nf.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          nf_conntrack
    - path: /etc/sysctl.d/nf.conf
      filesystem: root
      mode: 0644
      contents:
        inline: |
          net.netfilter.nf_conntrack_max=50000
EOF
      ,
    ]
  }

  # Adds oidc flags to API server with default values.
  # Acts as a smoke test to check if API server is functional after addition
  # of extra flags.
  oidc {}

  # Disable kube-proxy setting net.netfilter.nf_conntrack_max so we can
  # set it per worker pool via CLC snippet.
  conntrack_max_per_core = 0
}

component "metrics-server" {}

component "openebs-operator" {}

# openebs-storage-class component should always be the last to be installed
# pending when https://github.com/kinvolk/lokoctl/issues/374 is fixed
# because when the discovery failure for creating StoragePoolClaim happens,
# lokoctl install errors out and moves on with running the tests
# causing subsequent components below this to be skipped.
component "openebs-storage-class" {
  storage-class "openebs-test-sc" {
    replica_count = 1
    default       = true
  }
}

component "prometheus-operator" {
  grafana {
    secret_env = {
      "LOKOMOTIVE_VERY_SECRET_PASSWORD" = "VERY_VERY_SECRET"
    }
  }
  prometheus {
    ingress {
      host                       = "prometheus.$CLUSTER_ID.$AWS_DNS_ZONE"
      certmanager_cluster_issuer = "letsencrypt-staging"
    }
  }
}

component "experimental-linkerd" {
  enable_monitoring   = true
}

component "contour" {
  enable_monitoring = true
  service_type      = "NodePort"
}

component "cert-manager" {
  email           = "$EMAIL"
  service_monitor = true
}

component "external-dns" {
  policy   = "sync"
  owner_id = "$CLUSTER_ID"
  aws {
    zone_id               = "$AWS_DNS_ZONE_ID"
    aws_access_key_id     = "$AWS_ACCESS_KEY_ID"
    aws_secret_access_key = "$AWS_SECRET_ACCESS_KEY"
  }

  service_monitor = true
}

component "dex" {
  ingress_host = "$DEX_INGRESS_HOST"

  issuer_host = "$ISSUER_HOST"

  certmanager_cluster_issuer = "letsencrypt-staging"

  connector "github" {
    id   = "github"
    name = "GitHub"

    config {
      client_id     = "$GITHUB_CLIENT_ID"
      client_secret = "$GITHUB_CLIENT_SECRET"
      redirect_uri  = "$REDIRECT_URI"

      team_name_field = "slug"

      org {
        name = "kinvolk"
        teams = [
          "lokomotive-developers",
        ]
      }
    }
  }

  static_client {
    name   = "gangway"
    id     = "$DEX_STATIC_CLIENT_CLUSTERAUTH_ID"
    secret = "$DEX_STATIC_CLIENT_CLUSTERAUTH_SECRET"

    redirect_uris = ["$GANGWAY_REDIRECT_URL"]
  }
}

component "gangway" {
  cluster_name = "$CLUSTER_ID"

  ingress_host = "$GANGWAY_INGRESS_HOST"

  certmanager_cluster_issuer = "letsencrypt-staging"

  session_key = "$GANGWAY_SESSION_KEY"

  api_server_url = "$API_SERVER_URL"

  authorize_url = "$AUTHORIZE_URL"

  token_url = "$TOKEN_URL"

  client_id     = "$DEX_STATIC_CLIENT_CLUSTERAUTH_ID"
  client_secret = "$DEX_STATIC_CLIENT_CLUSTERAUTH_SECRET"

  redirect_url = "$GANGWAY_REDIRECT_URL"
}

component "rook" {
  enable_monitoring = true
}

component "flatcar-linux-update-operator" {}

component "httpbin" {
  ingress_host = "httpbin.$CLUSTER_ID.$AWS_DNS_ZONE"

  certmanager_cluster_issuer = "letsencrypt-staging"
}

component "aws-ebs-csi-driver" {
  // Avoid proliferation of unused PVs.
  reclaim_policy = "Delete"
}

component "experimental-istio-operator" {
  enable_monitoring = true
}

component "web-ui" {
  ingress {
    host                       = "web-ui.$CLUSTER_ID.$AWS_DNS_ZONE"
    certmanager_cluster_issuer = "letsencrypt-staging"
  }
  oidc {
    client_id     = "$DEX_STATIC_CLIENT_CLUSTERAUTH_ID"
    client_secret = "$DEX_STATIC_CLIENT_CLUSTERAUTH_SECRET"
    issuer_url    = "$ISSUER_HOST"
  }
}

component "inspektor-gadget" {}

component "node-problem-detector" {
  service_monitor = true
}
